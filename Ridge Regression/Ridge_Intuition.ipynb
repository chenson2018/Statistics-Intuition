{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Intuition Regarding Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of when you think of the Lasso and Ridge methods of regression? Do you have a mental picture in your head? Do you understand with graphical intuition how we get from Ordinary Least Squares to each of these methods?\n",
    "\n",
    "For any field of mathematics that is complicated enough, the answer usually becomes a resounding \"no\" to these questions. In this short notebook, I'll attempt to shead a little light on ways that we can gain intuition for these methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's going to be very important to keep a consistent view of our notation in order to understand the concepts that follow. I use the following conventions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we have n observations of p variables. I will write the following\n",
    "\n",
    "Response variable: \n",
    "\n",
    "$$\n",
    "\\textbf{y} = \n",
    "\\begin{bmatrix} \n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Explanatory variables:\n",
    "\n",
    "$$\n",
    "\\textbf{x} = \n",
    "\\begin{bmatrix} \n",
    "x_{1,1} & x_{1,2} & x_{1,3} & \\dots & x_{1,p}\\\\\n",
    "x_{2,1} & x_{2,2} & x_{2,3} & \\dots & x_{2,p}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "x_{n,1} & x_{n,2} & x_{n,3} & \\dots & x_{n,p} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Explanatory variables with constant column:\n",
    "\n",
    "$$\n",
    "\\textbf{X} = \n",
    "\\begin{bmatrix} \n",
    "1 & x_{1,1} & x_{1,2} & x_{1,3} & \\dots & x_{1,p}\\\\\n",
    "1& x_{2,1} & x_{2,2} & x_{2,3} & \\dots & x_{2,p}\\\\\n",
    "1 & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "1 & x_{n,1} & x_{n,2} & x_{n,3} & \\dots & x_{n,p} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Scaled explanatory variables:\n",
    "\n",
    "$$\n",
    "\\textbf{Z} = \n",
    "\\begin{bmatrix} \n",
    "z_{1,1} & z_{1,2} & z_{1,3} & \\dots & z_{1,p}\\\\\n",
    "z_{2,1} & z_{2,2} & z_{2,3} & \\dots & z_{2,p}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "z_{n,1} & z_{n,2} & z_{n,3} & \\dots & z_{n,p} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Coefficients:\n",
    "\n",
    "$$\n",
    "\\beta = \n",
    "\\begin{bmatrix} \n",
    "\\beta_0\\\\\n",
    "\\beta_1\\\\\n",
    "\\vdots \\\\\n",
    "\\beta_p\n",
    "\\end{bmatrix}\n",
    "\\hspace{10pt}\n",
    "\\text{or}\n",
    "\\hspace{10pt}\n",
    "\\beta = \n",
    "\\begin{bmatrix} \n",
    "\\beta_1\\\\\n",
    "\\beta_2\\\\\n",
    "\\vdots \\\\\n",
    "\\beta_p\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Error terms:\n",
    "\n",
    "$$\n",
    "\\textbf{e} = \n",
    "\\begin{bmatrix} \n",
    "\\epsilon_1\\\\\n",
    "\\epsilon_2\\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following equation:\n",
    "    \n",
    "$\\textbf{y} = \\textbf{X}\\beta + \\textbf{e}$\n",
    "\n",
    "where $\\textbf{e}$ represents error terms that are assumed to be normally distributed.\n",
    "\n",
    "This one equation represents regression values for **all of our data points** in one succicent line. It is the same as writing out the system of equations:\n",
    "\n",
    "\\begin{align*}\n",
    "y_1 &= \\beta_0 + \\beta_1 x_{1,1} + \\beta_2 x_{1,2} + \\beta_1 x_{1,3} + \\dots \\beta_p x_{1,p}\\\\\n",
    "y_2 &= \\beta_0 + \\beta_1 x_{2,1} + \\beta_2 x_{2,2} + \\beta_1 x_{2,3} + \\dots \\beta_p x_{2,p}\\\\\n",
    "& \\vdots \\\\\n",
    "y_n &= \\beta_0 + \\beta_1 x_{n,1} + \\beta_2 x_{n,2} + \\beta_1 x_{n,3} + \\dots \\beta_p x_{n,p}\n",
    "\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think for a moment about how crazy this is! With the power of linear algebra, we were able to condense this system of equations to just a few symbols.\n",
    "\n",
    "Now I want to find a way to interpret in linear algebra the same minimization of mean squared error. First notice that our errors are given by:\n",
    "\n",
    "\\begin{align*}\n",
    "\\epsilon_1 &= y_1 - (\\beta_0 + \\beta_1 x_{1,1} + \\beta_2 x_{1,2} + \\beta_1 x_{1,3} + \\dots \\beta_p x_{1,p})\\\\\n",
    "\\epsilon_2 &= y_2 - (\\beta_0 + \\beta_1 x_{2,1} + \\beta_2 x_{2,2} + \\beta_1 x_{2,3} + \\dots \\beta_p x_{2,p})\\\\\n",
    "& \\vdots \\\\\n",
    "\\epsilon_n &= y_n - (\\beta_0 + \\beta_1 x_{n,1} + \\beta_2 x_{n,2} + \\beta_1 x_{n,3} + \\dots \\beta_p x_{n,p})\n",
    "\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then our residual sum of squares is exactly:\n",
    "\n",
    "$$RSS(\\beta) = \\sum_{i=1}^n \\epsilon_i^2$$\n",
    "\n",
    "which in terms of our error matrix can be written:\n",
    "\n",
    "$$RSS(\\beta) = \\textbf{e}^T \\textbf{e}$$\n",
    "\n",
    "which expanded gives:\n",
    "\n",
    "\\begin{align*}\n",
    "RSS(\\beta) &= (\\textbf{y} - \\textbf{X} \\beta)^T (\\textbf{y} - \\textbf{X} \\beta) \\\\\n",
    "&= (\\textbf{y}^T -  \\beta^T\\textbf{X}^T) (\\textbf{y} - \\textbf{X} \\beta) \\\\\n",
    "&= \\textbf{y}^T\\textbf{y} - \\textbf{y}^T\\textbf{X}\\beta - \\beta^T\\textbf{X}^T\\textbf{y} +\\beta^T\\textbf{X}^T\\textbf{X}\\beta \\\\\n",
    "&= \\textbf{y}^T\\textbf{y} - 2\\beta^T\\textbf{X}^T\\textbf{y} +\\beta^T\\textbf{X}^T\\textbf{X}\\beta \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now taking the gradient to find our minimum:\n",
    "    \n",
    "\\begin{align*}\n",
    "0 = \\nabla RSS(\\beta) &= \\nabla\\textbf{y}^T\\textbf{y} - 2\\nabla\\beta^T\\textbf{X}^T\\textbf{y} +\\nabla\\beta^T\\textbf{X}^T\\textbf{X}\\beta \\\\\n",
    "0 &= - 2\\textbf{X}^T\\textbf{y} +2\\textbf{X}^T\\textbf{X}\\beta \\\\\n",
    "0 &= \\textbf{X}^T\\textbf{X}\\beta - \\textbf{X}^T\\textbf{y} \\\\\n",
    "\\textbf{X}^T\\textbf{X}\\beta &= \\textbf{X}^T\\textbf{y} \\\\\n",
    "\\hat \\beta &= (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using just linear algebra, we have determined the general solution for Ordinary least Squares! In fact, we could verify that all the same relationships exist as in our usual formulation. (Again there is some subtlety invloved with the intercept term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a solid interpretation of Ordinary Least Squares within a linear algebra context, let's try to extend this intuition to ridge regression\n",
    "\n",
    "First let's make a small change to exclude the intercept term we had in OLS:\n",
    "\n",
    "Coefficients:\n",
    "\n",
    "$$\n",
    "\\beta = \n",
    "\\begin{bmatrix} \n",
    "\\beta_1\\\\\n",
    "\\beta_2\\\\\n",
    "\\vdots \\\\\n",
    "\\beta_p\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Written with matrices, ridge regrssion is the solution to minimizing the \"Penalized Residual Sum of Squares\":\n",
    "\n",
    "$$PRSS\\big(\\beta\\big)_{\\ell_2} = (\\textbf{y}-\\textbf{Z}\\beta)^T(\\textbf{y}-\\textbf{Z}\\beta) + \\lambda\\|\\beta\\|_2^2$$\n",
    "\n",
    "Notice that this is exactly the same as in the OLS case, but with an added penalty on the size of our coefficients. This is exactly the same as writing:\n",
    "\n",
    "$$PRSS\\big(\\beta\\big)_{\\ell_2} = \\textbf{e}^T \\textbf{e} + \\lambda\\|\\beta\\|_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually find this solution, I simply need to take a derivative and set equal to zero:\n",
    "\n",
    "\\begin{align*}\n",
    "0 &= \\frac{\\partial PRSS\\big(\\beta\\big)_{\\ell_2}}{\\partial\\beta}\\\\\n",
    "&= -2\\textbf{Z}^T (\\textbf{y}-\\textbf{Z}\\beta) + 2\\lambda\\beta\n",
    "\\end{align*}\n",
    "\n",
    "which gives:\n",
    "\n",
    "$$\n",
    "\\hat \\beta_\\lambda^{ridge} = (\\textbf{Z}^T\\textbf{Z} + \\lambda \\textbf{I}_p)^{-1} \\textbf{Z}^T\\textbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this to the OLS solution, we can see that the difference can be quantified by adding $\\lambda$ down the diagonal of $\\textbf{Z}^T\\textbf{Z}$ in the OLS solution. \n",
    "\n",
    "This also helps quantify the idea that as $\\lambda$ is close to zero thew closer we are to OLS, and the closer $\\lambda$ is to infinity the closer we are to only capturing the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little reworking of our formula for the penalized residual sum of squares, we can actually reduce ridge regression to OLS on another data set\n",
    "\n",
    "Consider\n",
    "\n",
    "$$PRSS\\big(\\beta\\big)_{\\ell_2} = \\sum_{i=1}^n(y_i-z_i^\\intercal\\beta)^2 + \\lambda\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "rewritten as:\n",
    "\n",
    "$$PRSS\\big(\\beta\\big)_{\\ell_2} = \\sum_{i=1}^n(y_i-z_i^\\intercal\\beta)^2 + \\sum_{j=1}^p (0-\\sqrt{\\lambda}\\beta_j)^2$$\n",
    "\n",
    "We can interpret this as adding new points to our scaled observations giving:\n",
    "\n",
    "$$\n",
    "\\textbf{Z}_\\lambda = \n",
    "\\begin{bmatrix} \n",
    "z_{1,1} & z_{1,2} & z_{1,3} & \\dots & z_{1,p}\\\\\n",
    "z_{2,1} & z_{2,2} & z_{2,3} & \\dots & z_{2,p}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "z_{n,1} & z_{n,2} & z_{n,3} & \\dots & z_{n,p} \\\\\n",
    "\\sqrt{\\lambda} & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & \\sqrt{\\lambda} & 0 & \\ddots & 0 \\\\\n",
    "0 & 0 & \\sqrt{\\lambda} & \\ddots & 0 \\\\\n",
    "0 & 0 & 0 & \\ddots & 0 \\\\\n",
    "0 & 0 & 0 & 0 & \\sqrt{\\lambda} \\\\\n",
    "\\end{bmatrix}\n",
    ";\n",
    "\\textbf{y}_\\lambda = \n",
    "\\begin{bmatrix} \n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots \\\\\n",
    "y_n\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More concisely we can write:\n",
    "\n",
    "$$\n",
    "\\textbf{Z}_\\lambda = \n",
    "\\begin{bmatrix} \n",
    "\\textbf{Z}\\\\\n",
    "\\sqrt{\\lambda}\\textbf{I}_p\n",
    "\\end{bmatrix}\n",
    "\\hspace{5pt}\n",
    "\\textbf{y}_\\lambda = \n",
    "\\begin{bmatrix} \n",
    "\\textbf{y}\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now taking least squares of this:\n",
    "\n",
    "\\begin{align*}\n",
    "(\\textbf{Z}_\\lambda^T\\textbf{Z}_\\lambda)^{-1}\\textbf{Z}_\\lambda^T\\textbf{y}_\\lambda\n",
    "&= \\left( [\\textbf{Z}^T, \\sqrt{\\lambda}\\textbf{I}_p]\n",
    "\\begin{bmatrix} \n",
    "\\textbf{Z}\\\\\n",
    "\\sqrt{\\lambda}\\textbf{I}_p\n",
    "\\end{bmatrix}\n",
    "\\right)^{-1}\n",
    "[\\textbf{Z}^T, \\sqrt{\\lambda}\\textbf{I}_p]\n",
    "\\begin{bmatrix} \n",
    "\\textbf{y}\\\\\n",
    "0\n",
    "\\end{bmatrix} \\\\\n",
    "&= (\\textbf{Z}^T\\textbf{Z} + \\lambda \\textbf{I}_p)^{-1} \\textbf{Z}^T\\textbf{y}\n",
    "\\end{align*}\n",
    "\n",
    "which is exactly ridge regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we interpret this? Instead of thinking of thinking of ridge regression as a penalty for large beta values, we could think of it simply as OLS where we try adding a single point on each axis and seeing how the regression responds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression based on Coefficient Distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make a complete shift in our mode of thinking. Recall the log likelihood function for linear regression:\n",
    "\n",
    "\\begin{align*}\n",
    "{L({\\bf \\beta}|{\\bf y})} &:= P({\\bf y} | {\\bf \\beta}) \\\\\n",
    "    &= \\prod_{i=1}^{n} P_Y(y_i|{\\bf \\beta}, \\sigma^2) \\\\\n",
    "    &= \\prod_{i=1}^{n} \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(y_i- (\\beta_0 + \\beta_1 x_{i,1} + \\dots + \\beta_p x_{i,p}))^2}{2\\sigma^2}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at the maximum a posteriori probability estimate:\n",
    "\n",
    "\\begin{align*}\n",
    "{\\bf \\hat{\\beta}_{\\text{MAP}}} &= \\arg\\max_{\\bf \\beta} P(\\beta | y) \\\\\n",
    "&= \\arg\\max_{\\bf \\beta} \\frac{P(y | \\beta) P(\\beta)}{P(y)} \\\\\n",
    "&= \\arg\\max_{\\bf \\beta} P(y | \\beta) P(\\beta) \\\\\n",
    "&= \\arg\\max_{\\bf \\beta} \\log(P(y | \\beta) P(\\beta)) \\\\\n",
    "&= \\arg\\max_{\\bf \\beta} \\log P(y | \\beta) + \\log P(\\beta)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that each $\\beta_i$ is normally distributed with zero mean this gives:\n",
    "\n",
    "\\begin{align*}\n",
    " &\\arg\\max_{\\bf \\beta} \\Big[ \\log \\prod_{i=1}^{n} \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(y_i- (\\beta_0 + \\beta_1 x_{i,1} + ... + \\beta_p x_{i,p}))^2}{2\\sigma^2}}\n",
    " + \\log \\prod_{j=0}^{p} \\frac{1}{\\tau\\sqrt{2\\pi}}e^{-\\frac{\\beta_j^2}{2\\tau^2}} \\Big] \\\\\n",
    "&= \\arg\\max_{\\bf \\beta} \\Big[- \\sum_{i=1}^{n} {\\frac{(y_i- (\\beta_0 + \\beta_1 x_{i,1} + ... + \\beta_p x_{i,p}))^2}{2\\sigma^2}}\n",
    " - \\sum_{j=0}^{p} {\\frac{\\beta_j^2}{2\\tau^2}} \\Big]\\\\\n",
    "&= \\arg\\min_{\\bf \\beta} \\frac{1}{2\\sigma^2} \\big[ \\sum_{i=1}^{n} (y_i-(\\beta_0 + \\beta_1 x_{i,1} + ... + \\beta_p x_{i,p}))^2\n",
    " + \\frac{\\sigma^2}{\\tau^2} \\sum_{j=0}^{p} \\beta_j^2 \\big] \\\\\n",
    "&= \\arg\\min_{\\bf \\beta} \\big[ \\sum_{i=1}^{n} (y_i-(\\beta_0 + \\beta_1 x_{i,1} + ... + \\beta_p x_{i,p}))^2 + \\lambda \\sum_{j=0}^{p} \\beta_j^2 \\big]\n",
    "\\end{align*}\n",
    "\n",
    "and again, we see htat this is precisely ridge regression!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf\n",
    "\n",
    "https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/13/lecture-13.pdf\n",
    "\n",
    "http://bjlkeng.github.io/posts/probabilistic-interpretation-of-regularization/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
