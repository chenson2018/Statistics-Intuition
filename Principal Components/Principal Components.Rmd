---
title: "Principal Components"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
options(digits=5)
```

In this notebook, I will explore some aspects of principal components analysis that are not immediately obvious and review the basics of this method. We will use a small fabricated dataset of rent in Austin, where our goal is to predict rent prices based on eight explanatory variables:

```{r}
df = read.csv("AustinApartmentRent.csv", fileEncoding = "UTF-8-BOM")
X = df[,-1]
y = df[,1]
```

```{r}
head(df)
```

First, I will run the principal components command in R, selecting the option to scale and center our data:

```{r}
austin.pca = prcomp(X, center = TRUE, scale. = TRUE)
```

Let's be very clear about what is stored in the object that we have just created.

First, we have what are usually refered to as the "loadings" of PCA. This matrix of values represents the linear transformation that takes our original data to its new space. I will refer to this matrix as $L$

```{r}
L = austin.pca$rotation
L
```

We also have the transformed data itself, which I refer to as P:

```{r}
P = austin.pca$x
head(P)
```

This is our data, now transformed into a new coordinate system by the above loadings. With the loadings and our original data, we can perform this calculation manually:

```{r}
head(as.matrix(scale(X)) %*% L)
```

\newpage

What is special about this particular transformation is that is orthonormal, meaning it preserves all lengths and angles (i.e. the inner product) and more specifically, maximizes the variance among each principal component. We can visualize this by comparing the correlation matrix of our original data with our tansformed principal components.

```{r}
X.scale = scale(X)
round(cor(X.scale), digits = 5)
```

```{r}
round(cor(P), digits = 5)
```

\newpage

In the original data, there is correlation between our different variables, as would be expected. PCA takes the original data and "rotates" it to the point where there is no correlation between each direction, maximizing the variance in each new coordinate. We can also look at the sample covariance matrix to understand how much variance each principal components contains:

$$
\dfrac{1}{n-1}(P^TP)
$$

in R this is:

```{r}
(1/59)*round(t(P) %*% P, digits = 5)
```

What we see is that our transformation has eliminated any correlation between our considered variables, as shown by the fact that entries only appear in the diagonals of our sample covariance matrix. In fact, these are what our refered to as the eigenvalues of our PCA, which represent the amount of variance in each principal component:

```{r}
austin.pca$sdev^2
```

Since these are the elements of the diagonal covariance matrix, we can see that these eigenvalues are also equal to the amount of variance in each principal component, which are ordered by decreasing variance.

\newpage

With all this in mind, we can now look at regression using these principal components:

```{r}
data  = as.data.frame(cbind(y, austin.pca$x))
model.pca.full = lm(y ~ ., data = data)
summary(model.pca.full)
```

\newpage

Notice that our standard errors of the coefficients of this regression are strictly increasing. This is because:

$$
\widehat{\operatorname{var}}(\hat{\beta}) = s^2(X^TX)^{-1}
$$

where s is the standard error of our residuals. In the case of principal components, we saw that the term $X^TX$ is equal to exactly the diagonal matrix of eigenvalues multiplied by (n-1):

$$
\widehat{\operatorname{var}}(\hat{\beta}) = s^2
  \begin{bmatrix}
    (n-1)\lambda_{1} & & \\
    & \ddots & \\
    & & (n-1)\lambda_{j}
  \end{bmatrix}^{-1}
$$
Simplifying:

$$
\widehat{\operatorname{var}}(\hat{\beta}) =
  \begin{bmatrix}
    \dfrac{s^2}{(n-1)\lambda_{1}} & & \\
    & \ddots & \\
    & & \dfrac{s^2}{(n-1)\lambda_{j}}
  \end{bmatrix}
$$

recalling that 

$$
\lambda_1 \geq \dots \geq \lambda_j
$$

we can see that each successive variance is increasing, thus explaining why the variance of coefficients is increasing in our regression. It is interesting to note that this is independent of the response variable! 

We could alternatively note that:

$$
\widehat{\operatorname{var}}(\hat{\beta}_j) = \frac{s^2}{(n-1)\widehat{\operatorname{var}}(X_j)}\cdot \frac{1}{1-R_j^2}
$$

where $R_J^2$ is the is the multiple $R^2$ for the regression of $X_j$ on the other covariates. However, we already established that the variance of each principal component is its corresponding eigenvalue, and that this $R^2$ value is zero. So we have: 

$$
\widehat{\operatorname{var}}(\hat{\beta}_j) = \frac{s^2}{(n-1)\lambda_j}
$$

\newpage

We can compute this in R:

```{r}
sum = summary(model.pca.full)
s = sum$sigma
eigenvalues = austin.pca$sdev^2
```


```{r}
sqrt(s^2/(59*eigenvalues))
```

And see that we get the same standard errors that R calculated in the above regression:

```{r}
sum$coefficients[-1,2]
```

\newpage

Finally, we can graphically view how these differing variances of our coefficients appear in a simulation. Below I have have 1000 random samples of our data:


```{r}
model.pca.full.coef = matrix(data = 0, nrow = 1000, ncol = 8)


for (i in 1:1000){
  sample  = sample(1:60, 40)
  model.pca.full.coef[i,] = as.numeric(summary(lm(y ~ ., data = data[sample,]))$coefficients[-1,1])

}
```

Below I plot a histogram of our esimate of each coefficient for each of our simulations, centering the data so that we can compare:

```{r}
model.pca.full.coef = as.data.frame(model.pca.full.coef)
colnames(model.pca.full.coef) = paste("PC", 1:8, sep = "")

library(ggplot2)
pca.center = as.data.frame(apply(model.pca.full.coef, 2, scale, scale = FALSE, center = TRUE))
ggplot(stack(pca.center), aes(x=values, fill = ind))+ geom_histogram(binwidth = 2, color = "black")
```

We see the expected result, that as we look at each successive coefficient of our principal components that we observe a higher varience for our estimate.

\newpage

For what may be a more clear picture, we can compare PC1, PC6, and PC8:

```{r}
ggplot(stack(pca.center)[c(1:1000, 5001:6000, 7001:8000),], aes(x=values, fill = ind))+ geom_histogram(binwidth = 2, color = "black")
```

